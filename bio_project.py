import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn import ensemble
from sklearn import naive_bayes
from sklearn.neighbors.nearest_centroid import NearestCentroid
from sklearn import linear_model
from sklearn import neighbors
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report
# from sklearn.metrics import precision_recall_fscore_support
from sklearn.preprocessing import normalize
from sklearn.preprocessing import scale
from sklearn.feature_selection import VarianceThreshold

from sklearn import metrics as skm

from keras.models import Sequential
from keras.layers import Dense
from keras.layers.core import Dropout
from keras.layers import LSTM

features = '/Users/rohan/Documents/Grad School/Coursework/Winter/biosignals/newnew_final.txt'

class Classifier(object):

    def __init__(self, path):
        self.path = path

    def read_data(self):
        df = pd.read_csv(features, delimiter='\t').interpolate()
        return np.asarray(df).reshape(9, 27000)

    def create_feat_and_labels(self):
        y = self.read_data()[2].astype(int)
        x = self.read_data()[3:]
        return x, y


class UseClassifier(object):

    def __init__(self, X):
        self.X, self.y = X
        self.X = self.X.reshape(27000, 6)
        # try_this = np.array(self.X).astype(np.float)
        # scaled_val = scale(try_this)
        # # print(f'after...{try_this_new.mean(axis = 0)}')
        # pca = PCA()
        # self.X = pca.fit_transform(scaled_val)
        # print(f'PCA variance ratio of features: {np.round_(pca.explained_variance_ratio_, decimals=2)}')

    def linear_classifier(self):
        clf = svm.SVC(kernel='sigmoid', verbose=3)
        print(self.X.shape, self.y.shape)
        # X's are a numpy array of features (list of list where first list is the row of features
        # corresponding to the first label, and subsequent rows correspond to subsequent features
        # x would have each feature generated by the initial signal hook_ch1[0], etc,.
        # y is the label (this would be the type of grasp - Hook, tip, etc,.)
        clf.fit(self.X, self.y.ravel())

        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.30, random_state=42)

        return clf.predict(X_test), y_test

    def random_forest_classifier(self):
        clf = ensemble.RandomForestClassifier(n_estimators=30, max_depth=30, random_state=40, verbose=3)
        clf.fit(self.X, self.y.ravel())
        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.30, random_state=42)

        # TODO: Mention in paper that we used KFold to see how good our model was.
        # kf = StratifiedKFold(n_splits=5)
        # kf.get_n_splits(self.X)
        # for train_index, test_index in kf.split(self.X, self.y):
        #     X_train, X_test = self.X[train_index], self.X[test_index]
        #     y_train, y_test = self.y[train_index], self.y[test_index]

        return clf.predict(X_test), y_test

    def adaboost_classifier(self):
        print('Running Adaboost Classifier with test_size = 0.30, random_state = 42')
        clf = ensemble.AdaBoostClassifier()
        clf.fit(self.X, self.y.ravel())
        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.30, random_state=42)

        return clf.predict(X_test), y_test

    # def centroid_classifier(self):
    #     clf = NearestCentroid()
    #     a = np.array(self.X).astype(np.float)
    #     b = np.array(self.y).astype(np.float)
    #     clf.fit(a, b.ravel())
    #     X_train, X_test, y_train, y_test = train_test_split(a, b, test_size = 0.30, random_state = 42)
    #
    #     return clf.predict(X_test), y_test

    def gaussian_naive_bayes(self):
        print('Running Gaussian Naive Bayes with test_size = 0.30, random_state = 42...')
        clf = naive_bayes.GaussianNB()
        a = np.array(self.X).astype(np.float)
        b = np.array(self.y).astype(np.float)
        clf.fit(a, b.ravel())
        X_train, X_test, y_train, y_test = train_test_split(a, b, test_size=0.30, random_state=42)

        return clf.predict(X_test), y_test

    # def ridge_classifier(self):
    #     clf = linear_model.RidgeClassifier()
    #     a = np.array(self.X).astype(np.float)
    #     b = np.array(self.y).astype(np.float)
    #     clf.fit(a, b.ravel())
    #     X_train, X_test, y_train, y_test = train_test_split(a, b, test_size = 0.30, random_state = 42)
    #
    #     return clf.predict(X_test), y_test

    def k_neighbors(self):
        print('Running KNeighborsClassifier with n = 2, test_size = 0.30, random_state = 42...')
        clf = neighbors.KNeighborsClassifier(n_neighbors=10)  # see bias-variance tradeoff because n=1; acc=1
        a = np.array(self.X).astype(np.float)
        b = np.array(self.y).astype(np.float)
        clf.fit(a, b.ravel())
        X_train, X_test, y_train, y_test = train_test_split(a, b, test_size=0.30, random_state=42)

        return clf.predict(X_test), y_test

    # def perceptron(self):
    #     clf = linear_model.Perceptron()
    #     a = np.array(self.X).astype(np.float)
    #     b = np.array(self.y).astype(np.float)
    #     clf.fit(a, b.ravel())
    #     X_train, X_test, y_train, y_test = train_test_split(a, b, test_size = 0.30, random_state = 42)
    #     # print(clf.score(X_test, y_test))
    #     return clf.predict(X_test), y_test

    # def neural_network(self):
    #     model = Sequential()
    #     model.add(Dense(50, input_dim=6, activation='relu'))
    #     model.add(Dense(25, activation='relu'))
    #     model.add(Dense(6, activation='relu'))
    #     model.add((Dense(1,activation='sigmoid')))
    #     model.add()
    #     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    #     model.fit(self.X, self.y, epochs=150, batch_size=500)
    #     scores = model.evaluate(self.X, self.y)
    #     print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1] * 100))
    #     predictions = model.predict_classes(self.X)
    #     # rounded = [round(x[0]) for x in predictions]
    #     # print(rounded)
    #     print(predictions)

    def build_model(self):
        data_dim = 6
        timesteps = 1

        model = Sequential()
        model.add(LSTM(30, return_sequences=True,
                       input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 30
        model.add(LSTM(30, return_sequences=True))  # returns a sequence of vectors of dimension 30
        model.add(LSTM(30))  # return a single vector of dimension 30
        model.add(Dense(10, activation='relu'))
        model.add(Dense(1, activation='softmax'))

        model.compile(loss='binary_crossentropy',
                      optimizer='adam',
                      metrics=['accuracy'])

        model.summary()
        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.30, random_state=42)
        X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
        X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))
        model.fit(X_train, y_train, batch_size=400, epochs=20, verbose=1)
        scores = model.evaluate(X_test, y_test)
        print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1] * 100))
        predictions = model.predict_classes(X_test)
        print(predictions)

def predict_metrics(x):
    y_pred, y_true = x
    # print(f'Precision, Recall and F1 score are... \n'
    #       f'{classification_report(y_true, y_pred, target_names=classes.keys())}')
    print('The accuracy is: ', skm.accuracy_score(y_true, y_pred))
    # print('The precision is: ',skm.precision_score(y_true, y_pred))
    # print('The recall is: ', skm.recall_score(y_true, y_pred))
    # print('The F1 score for each class is: ', skm.f1_score(y_true, y_pred, average=None))
    # skm.roc_auc_score(x)
    # plt = skm.roc_curve(y_true, y_pred)
    # plt.show()
    print('Confusion matrix:')
    print(skm.confusion_matrix(y_true, y_pred))


classify = Classifier(features)
classify.read_data()

use_classify = UseClassifier(classify.create_feat_and_labels())
x = use_classify.k_neighbors()
predict_metrics(x)
